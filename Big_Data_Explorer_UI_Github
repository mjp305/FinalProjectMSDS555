{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6775815",
        "outputId": "55635055-4daa-44eb-bb47-990e6a819d7f"
      },
      "source": [
        "%pip install neo4j"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: neo4j in /usr/local/lib/python3.11/dist-packages (5.28.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from neo4j) (2025.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ddfe7c9",
        "outputId": "ce8ad7ac-cf71-4e6b-8210-ef9c6ada761f"
      },
      "source": [
        "%pip install streamlit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.48.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8504!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49NKUlPEzl72",
        "outputId": "626f2362-cf52-4495-b395-14434c967436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0KUsage: lt --port [num] <options>\n",
            "\n",
            "Options:\n",
            "  -p, --port                Internal HTTP server port                 [required]\n",
            "  -h, --host                Upstream server providing forwarding\n",
            "                                             [default: \"https://localtunnel.me\"]\n",
            "  -s, --subdomain           Request this subdomain\n",
            "  -l, --local-host          Tunnel traffic to this host instead of localhost,\n",
            "                            override Host header to this host\n",
            "      --local-https         Tunnel traffic to a local HTTPS server     [boolean]\n",
            "      --local-cert          Path to certificate PEM file for local HTTPS server\n",
            "      --local-key           Path to certificate key file for local HTTPS server\n",
            "      --local-ca            Path to certificate authority file for self-signed\n",
            "                            certificates\n",
            "      --allow-invalid-cert  Disable certificate checks for your local HTTPS\n",
            "                            server (ignore cert/key/ca options)        [boolean]\n",
            "  -o, --open                Opens the tunnel URL in your browser\n",
            "      --print-requests      Print basic request info                   [boolean]\n",
            "      --help                Show this help and exit                    [boolean]\n",
            "      --version             Show version number                        [boolean]\n",
            "\n",
            "Invalid argument: `port` must be a number\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjKtwTfa0l8U",
        "outputId": "dd722f1b-5eda-4dd6-e2b0-7ad12bded217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.127.41.126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Step 1: Install requirements (run in terminal or notebook) ===\n",
        "# %pip install streamlit neo4j pandas matplotlib seaborn\n",
        "\n",
        "# === Step 2: Streamlit App ===\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from neo4j import GraphDatabase\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import BytesIO\n",
        "\n",
        "# === Spark session (must run on system with Spark installed) ===\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Big Data Explorer\") \\\n",
        "    .config(\"spark.jars\", \"/content/mysql-connector-java-8.0.28.jar\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# === MySQL JDBC Configuration ===\n",
        "jdbc_url = 'jdbc:mysql://localhost:3306/adultdb'\n",
        "properties = {\n",
        "    'user': 'root',\n",
        "    'password': 'root',\n",
        "    'driver': 'com.mysql.cj.jdbc.Driver'\n",
        "}\n",
        "\n",
        "# === Neo4j Configuration ===\n",
        "neo4j_uri = \"bolt://52.21.191.52:7687\"\n",
        "neo4j_user = \"neo4j\"\n",
        "neo4j_password = \"blue-tree-fun\"\n",
        "driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
        "\n",
        "def run_cypher_query(query):\n",
        "    with driver.session() as session:\n",
        "        result = session.run(query)\n",
        "        return [record.data() for record in result]\n",
        "\n",
        "# === Sidebar: Team Profiles ===\n",
        "st.sidebar.markdown(\"## 🧠 Project Contributors\")\n",
        "\n",
        "st.sidebar.markdown(\"\"\"\n",
        "<div style='text-align: center'>\n",
        "    <img src='Victoria Love Franklin.JPG' style='border-radius: 50%;'><br>\n",
        "    <strong>Victoria Love Franklin</strong><br>\n",
        "    Scientist, U.S. Dept. of Defense\n",
        "</div>\n",
        "<br>\n",
        "<div style='text-align: center'>\n",
        "    <img src='https://via.placeholder.com/100x100.png?text=Michael' style='border-radius: 50%;'><br>\n",
        "    <strong>Michael J. Paul</strong><br>\n",
        "    Professor, Data Science & Informatics\n",
        "</div>\n",
        "<br>\n",
        "<div style='text-align: center'>\n",
        "    <img src='https://via.placeholder.com/100x100.png?text=Emirrah' style='border-radius: 50%;'><br>\n",
        "    <strong>Emirrah Sanders</strong><br>\n",
        "    Data Scientist, Public Health Analytics\n",
        "</div>\n",
        "<br>\n",
        "<div style='text-align: center'>\n",
        "    <img src='https://via.placeholder.com/100x100.png?text=Courtney' style='border-radius: 50%;'><br>\n",
        "    <strong>Courtney Quarterman</strong><br>\n",
        "    AI & Data Science Educator\n",
        "</div>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# === App Layout ===\n",
        "st.set_page_config(layout=\"wide\")\n",
        "st.title(\"Big Data Explorer\")\n",
        "\n",
        "tech_choice = st.sidebar.radio(\"Choose Technology\", [\"Apache Spark\", \"Graph Database (Neo4j)\"])\n",
        "\n",
        "# === Apache Spark Section ===\n",
        "if tech_choice == \"Apache Spark\":\n",
        "    st.header(\"Apache Spark Operations\")\n",
        "    task = st.selectbox(\"Choose Task\", [\"View Top 5 Rows\", \"Count Age < 40\", \"Count by Education\", \"Load to MySQL and Verify\"])\n",
        "\n",
        "    df = spark.read.csv(\"adult.csv\", header=True, inferSchema=True)\n",
        "\n",
        "    if task == \"View Top 5 Rows\":\n",
        "        st.dataframe(df.limit(5).toPandas())\n",
        "        st.code(df._jdf.schema().treeString())\n",
        "\n",
        "    elif task == \"Count Age < 40\":\n",
        "        count = df.filter(col(\"age\") < 40).count()\n",
        "        st.success(f\"Number of people below 40 years old: {count}\")\n",
        "\n",
        "    elif task == \"Count by Education\":\n",
        "        edu_counts = df.groupBy(\"education\").count().orderBy(\"count\")\n",
        "        st.dataframe(edu_counts.toPandas())\n",
        "\n",
        "    elif task == \"Load to MySQL and Verify\":\n",
        "        df.write.mode(\"overwrite\").jdbc(url=jdbc_url, table=\"Adult\", properties=properties)\n",
        "        verified = spark.read.jdbc(url=jdbc_url, table=\"Adult\", properties=properties).limit(5)\n",
        "        st.dataframe(verified.toPandas())\n",
        "\n",
        "# === Neo4j Section ===\n",
        "elif tech_choice == \"Graph Database (Neo4j)\":\n",
        "    st.header(\"Neo4j Graph Explorer\")\n",
        "    query_option = st.selectbox(\"Select Query\", [\n",
        "        \"Count All Nodes\",\n",
        "        \"Create Ronaldinho & Brazil\",\n",
        "        \"Add FOOTBALLER_OF Relationship\",\n",
        "        \"Create Top Scorers & FIFA\",\n",
        "        \"Load CSV (FIFA.csv)\",\n",
        "        \"Top Scorers Chart\"\n",
        "    ])\n",
        "\n",
        "    if query_option == \"Count All Nodes\":\n",
        "        q = \"MATCH (n) RETURN count(n) AS nodeCount\"\n",
        "        st.json(run_cypher_query(q))\n",
        "\n",
        "    elif query_option == \"Create Ronaldinho & Brazil\":\n",
        "        q = '''\n",
        "        CREATE (p:Player {name: \"Ronaldo Gaúcho\", YOB: 1980, POB: \"Porto Alegre\"})\n",
        "        CREATE (c:Country {name: \"Brazil\"})\n",
        "        CREATE (p)-[:PLAYER_OF]->(c)\n",
        "        '''\n",
        "        run_cypher_query(q)\n",
        "        st.success(\"Nodes and relationship created\")\n",
        "\n",
        "    elif query_option == \"Add FOOTBALLER_OF Relationship\":\n",
        "        q = '''\n",
        "        MATCH (p:Player {name: 'Ronaldo Gaúcho'}), (c:Country {name: 'Brazil'})\n",
        "        CREATE (p)-[:FOOTBALLER_OF {Matches: 97, Goals: 33}]->(c)\n",
        "        '''\n",
        "        run_cypher_query(q)\n",
        "        st.success(\"FOOTBALLER_OF relationship added\")\n",
        "\n",
        "    elif query_option == \"Create Top Scorers & FIFA\":\n",
        "        q = '''\n",
        "        CREATE (wc:Event {name: \"FIFA World Cup\"});\n",
        "        CREATE (p1:Player {name: \"Miroslav Klose\", country: \"Germany\", goals: 16, matches: 24})\n",
        "        CREATE (p2:Player {name: \"Ronaldo\", country: \"Brazil\", goals: 15, matches: 19})\n",
        "        CREATE (p3:Player {name: \"Gerd Müller\", country: \"Germany\", goals: 14, matches: 13})\n",
        "        CREATE (p4:Player {name: \"Just Fontaine\", country: \"France\", goals: 13, matches: 6})\n",
        "        CREATE (p5:Player {name: \"Pelé\", country: \"Brazil\", goals: 12, matches: 14})\n",
        "        MATCH (wc:Event {name: \"FIFA World Cup\"})\n",
        "        MATCH (p1:Player {name: \"Miroslav Klose\"})\n",
        "        MATCH (p2:Player {name: \"Ronaldo\"})\n",
        "        MATCH (p3:Player {name: \"Gerd Müller\"})\n",
        "        MATCH (p4:Player {name: \"Just Fontaine\"})\n",
        "        MATCH (p5:Player {name: \"Pelé\"})\n",
        "        CREATE (p1)-[:PLAYED_AT]->(wc)\n",
        "        CREATE (p2)-[:PLAYED_AT]->(wc)\n",
        "        CREATE (p3)-[:PLAYED_AT]->(wc)\n",
        "        CREATE (p4)-[:PLAYED_AT]->(wc)\n",
        "        CREATE (p5)-[:PLAYED_AT]->(wc);\n",
        "        '''\n",
        "        run_cypher_query(q)\n",
        "        st.success(\"Top scorers and FIFA World Cup node created\")\n",
        "\n",
        "    elif query_option == \"Load CSV (FIFA.csv)\":\n",
        "        q = '''\n",
        "        LOAD CSV WITH HEADERS FROM 'file:///FIFA.csv' AS row\n",
        "        CREATE (:Player {\n",
        "          Rank: toInteger(row.Rank),\n",
        "          Name: row.Name,\n",
        "          Country: row.Country,\n",
        "          Goals: toInteger(row.Goals),\n",
        "          Matches: toInteger(row.Matches)\n",
        "        });\n",
        "        '''\n",
        "        run_cypher_query(q)\n",
        "        st.success(\"CSV data loaded into Neo4j\")\n",
        "\n",
        "    elif query_option == \"Top Scorers Chart\":\n",
        "        q = \"\"\"\n",
        "        MATCH (p:Player)\n",
        "        RETURN p.name AS Player, p.goals AS Goals, p.matches AS Matches\n",
        "        ORDER BY Goals DESC\n",
        "        \"\"\"\n",
        "        data = run_cypher_query(q)\n",
        "\n",
        "        if data:\n",
        "            df = pd.DataFrame(data)\n",
        "            st.subheader(\"⚽ Top FIFA World Cup Scorers: Goals vs. Matches\")\n",
        "\n",
        "            # Matplotlib Grouped Bar Chart\n",
        "            fig, ax = plt.subplots(figsize=(10, 6))\n",
        "            bar_width = 0.4\n",
        "            index = range(len(df))\n",
        "\n",
        "            ax.bar(index, df[\"Goals\"], bar_width, label=\"Goals\", color='skyblue')\n",
        "            ax.bar([i + bar_width for i in index], df[\"Matches\"], bar_width, label=\"Matches\", color='lightgreen')\n",
        "\n",
        "            ax.set_xlabel(\"Player\")\n",
        "            ax.set_ylabel(\"Count\")\n",
        "            ax.set_title(\"Goals and Matches by Player\")\n",
        "            ax.set_xticks([i + bar_width / 2 for i in index])\n",
        "            ax.set_xticklabels(df[\"Player\"], rotation=45)\n",
        "            ax.legend()\n",
        "\n",
        "            st.pyplot(fig)\n",
        "\n",
        "            # Download CSV Button\n",
        "            csv = df.to_csv(index=False).encode()\n",
        "            st.download_button(\n",
        "                label=\"⬇️ Download Data as CSV\",\n",
        "                data=csv,\n",
        "                file_name=\"fifa_top_scorers.csv\",\n",
        "                mime='text/csv'\n",
        "            )\n",
        "        else:\n",
        "            st.warning(\"No data available. Run 'Create Top Scorers & FIFA' or 'Load CSV' first.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrXelawI7DNq",
        "outputId": "da407714-5337-47eb-b23c-5f21ab6f4daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-07 19:31:31.653 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.654 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.659 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.661 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.663 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.664 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.666 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.667 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.668 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.669 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.670 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.671 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.673 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.674 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.677 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.678 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.679 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.681 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.682 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.683 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.684 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.685 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.686 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.687 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.688 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.688 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:31.689 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:32.931 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:32.933 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:32.936 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:32.941 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:32.942 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-07 19:31:32.944 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-J-zGO20hwz",
        "outputId": "8dcc291a-ee3c-4b89-e0f8-212ae0538dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.127.41.126:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0Kyour url is: https://evil-baboons-eat.loca.lt\n",
            "25/08/07 19:32:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/08/07 19:32:27 WARN DependencyUtils: Local jar /content/mysql-connector-java-8.0.28.jar does not exist, skipping.\n",
            "25/08/07 19:32:27 INFO SparkContext: Running Spark version 3.5.1\n",
            "25/08/07 19:32:27 INFO SparkContext: OS info Linux, 6.1.123+, amd64\n",
            "25/08/07 19:32:27 INFO SparkContext: Java version 11.0.28\n",
            "25/08/07 19:32:27 INFO ResourceUtils: ==============================================================\n",
            "25/08/07 19:32:27 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/08/07 19:32:27 INFO ResourceUtils: ==============================================================\n",
            "25/08/07 19:32:27 INFO SparkContext: Submitted application: Big Data Explorer\n",
            "25/08/07 19:32:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/08/07 19:32:28 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/08/07 19:32:28 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/08/07 19:32:28 INFO SecurityManager: Changing view acls to: root\n",
            "25/08/07 19:32:28 INFO SecurityManager: Changing modify acls to: root\n",
            "25/08/07 19:32:28 INFO SecurityManager: Changing view acls groups to: \n",
            "25/08/07 19:32:28 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/08/07 19:32:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/08/07 19:32:28 INFO Utils: Successfully started service 'sparkDriver' on port 44851.\n",
            "25/08/07 19:32:28 INFO SparkEnv: Registering MapOutputTracker\n",
            "25/08/07 19:32:28 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/08/07 19:32:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/08/07 19:32:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/08/07 19:32:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/08/07 19:32:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c3b2bfc5-bdb8-4a70-91db-e387e60db8e3\n",
            "25/08/07 19:32:28 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "25/08/07 19:32:28 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/08/07 19:32:29 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/08/07 19:32:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/08/07 19:32:29 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/08/07 19:32:29 ERROR SparkContext: Failed to add /content/mysql-connector-java-8.0.28.jar to Spark environment\n",
            "java.io.FileNotFoundException: Jar /content/mysql-connector-java-8.0.28.jar not found\n",
            "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:2100)\n",
            "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2156)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$new$15(SparkContext.scala:526)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$new$15$adapted(SparkContext.scala:526)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:526)\n",
            "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
            "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
            "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
            "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "25/08/07 19:32:29 INFO Executor: Starting executor ID driver on host 09e87e5f6ba6\n",
            "25/08/07 19:32:29 INFO Executor: OS info Linux, 6.1.123+, amd64\n",
            "25/08/07 19:32:29 INFO Executor: Java version 11.0.28\n",
            "25/08/07 19:32:29 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/08/07 19:32:29 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@263ee82f for default.\n",
            "25/08/07 19:32:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35457.\n",
            "25/08/07 19:32:29 INFO NettyBlockTransferService: Server created on 09e87e5f6ba6:35457\n",
            "25/08/07 19:32:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/08/07 19:32:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 09e87e5f6ba6, 35457, None)\n",
            "25/08/07 19:32:29 INFO BlockManagerMasterEndpoint: Registering block manager 09e87e5f6ba6:35457 with 434.4 MiB RAM, BlockManagerId(driver, 09e87e5f6ba6, 35457, None)\n",
            "25/08/07 19:32:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 09e87e5f6ba6, 35457, None)\n",
            "25/08/07 19:32:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 09e87e5f6ba6, 35457, None)\n",
            "25/08/07 19:32:31 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/08/07 19:32:31 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/08/07 19:32:33 INFO InMemoryFileIndex: It took 95 ms to list leaf files for 1 paths.\n",
            "25/08/07 19:32:34 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
            "25/08/07 19:32:39 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/07 19:32:39 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
            "25/08/07 19:32:40 INFO CodeGenerator: Code generated in 509.969169 ms\n",
            "25/08/07 19:32:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)\n",
            "25/08/07 19:32:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)\n",
            "25/08/07 19:32:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 09e87e5f6ba6:35457 (size: 34.5 KiB, free: 434.4 MiB)\n",
            "25/08/07 19:32:40 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:32:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4760336 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/07 19:32:40 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:32:41 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "25/08/07 19:32:41 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
            "25/08/07 19:32:41 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/07 19:32:41 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:32:41 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/08/07 19:32:41 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 434.2 MiB)\n",
            "25/08/07 19:32:41 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.2 MiB)\n",
            "25/08/07 19:32:41 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 09e87e5f6ba6:35457 (size: 6.4 KiB, free: 434.4 MiB)\n",
            "25/08/07 19:32:41 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:32:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/07 19:32:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/08/07 19:32:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (09e87e5f6ba6, executor driver, partition 0, PROCESS_LOCAL, 8203 bytes) \n",
            "25/08/07 19:32:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/08/07 19:32:41 INFO CodeGenerator: Code generated in 62.147503 ms\n",
            "25/08/07 19:32:41 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 0-4760336, partition values: [empty row]\n",
            "25/08/07 19:32:41 INFO CodeGenerator: Code generated in 38.732705 ms\n",
            "25/08/07 19:32:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1755 bytes result sent to driver\n",
            "25/08/07 19:32:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 670 ms on 09e87e5f6ba6 (executor driver) (1/1)\n",
            "25/08/07 19:32:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:32:42 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.004 s\n",
            "25/08/07 19:32:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/07 19:32:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/08/07 19:32:42 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.128699 s\n",
            "25/08/07 19:32:42 INFO CodeGenerator: Code generated in 24.925509 ms\n",
            "25/08/07 19:32:42 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/07 19:32:42 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/07 19:32:42 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.1 KiB, free 434.0 MiB)\n",
            "25/08/07 19:32:42 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n",
            "25/08/07 19:32:42 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 09e87e5f6ba6:35457 (size: 34.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:32:42 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:32:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4760336 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/07 19:32:42 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:32:42 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
            "25/08/07 19:32:42 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
            "25/08/07 19:32:42 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/07 19:32:42 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:32:42 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/08/07 19:32:42 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 27.9 KiB, free 433.9 MiB)\n",
            "25/08/07 19:32:42 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 433.9 MiB)\n",
            "25/08/07 19:32:42 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 09e87e5f6ba6:35457 (size: 12.9 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:32:42 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:32:42 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/07 19:32:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
            "25/08/07 19:32:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (09e87e5f6ba6, executor driver, partition 0, PROCESS_LOCAL, 8203 bytes) \n",
            "25/08/07 19:32:42 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (09e87e5f6ba6, executor driver, partition 1, PROCESS_LOCAL, 8203 bytes) \n",
            "25/08/07 19:32:42 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/08/07 19:32:42 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
            "25/08/07 19:32:42 INFO CodeGenerator: Code generated in 27.35547 ms\n",
            "25/08/07 19:32:42 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 4760336-5326368, partition values: [empty row]\n",
            "25/08/07 19:32:42 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 0-4760336, partition values: [empty row]\n",
            "25/08/07 19:32:43 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1676 bytes result sent to driver\n",
            "25/08/07 19:32:43 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 649 ms on 09e87e5f6ba6 (executor driver) (1/2)\n",
            "25/08/07 19:32:43 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 09e87e5f6ba6:35457 in memory (size: 6.4 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:32:43 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 09e87e5f6ba6:35457 in memory (size: 34.5 KiB, free: 434.4 MiB)\n",
            "25/08/07 19:32:44 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1676 bytes result sent to driver\n",
            "25/08/07 19:32:44 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1923 ms on 09e87e5f6ba6 (executor driver) (2/2)\n",
            "25/08/07 19:32:44 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 2.018 s\n",
            "25/08/07 19:32:44 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/07 19:32:44 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:32:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/08/07 19:32:44 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 2.047206 s\n",
            "25/08/07 19:32:44 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/07 19:32:44 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/07 19:32:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.0 KiB, free 433.9 MiB)\n",
            "25/08/07 19:32:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n",
            "25/08/07 19:32:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 09e87e5f6ba6:35457 (size: 34.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:32:45 INFO SparkContext: Created broadcast 4 from toPandas at /content/app.py:45\n",
            "25/08/07 19:32:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4760336 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/07 19:32:45 INFO SparkContext: Starting job: toPandas at /content/app.py:45\n",
            "25/08/07 19:32:45 INFO DAGScheduler: Got job 2 (toPandas at /content/app.py:45) with 1 output partitions\n",
            "25/08/07 19:32:45 INFO DAGScheduler: Final stage: ResultStage 2 (toPandas at /content/app.py:45)\n",
            "25/08/07 19:32:45 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/07 19:32:45 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:32:45 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at toPandas at /content/app.py:45), which has no missing parents\n",
            "25/08/07 19:32:45 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 13.2 KiB, free 433.9 MiB)\n",
            "25/08/07 19:32:45 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 433.9 MiB)\n",
            "25/08/07 19:32:45 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 09e87e5f6ba6:35457 (size: 6.8 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:32:45 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:32:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at toPandas at /content/app.py:45) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/07 19:32:45 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
            "25/08/07 19:32:45 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3) (09e87e5f6ba6, executor driver, partition 0, PROCESS_LOCAL, 8203 bytes) \n",
            "25/08/07 19:32:45 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)\n",
            "25/08/07 19:32:45 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 0-4760336, partition values: [empty row]\n",
            "25/08/07 19:32:45 INFO CodeGenerator: Code generated in 127.562072 ms\n",
            "25/08/07 19:32:45 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 2130 bytes result sent to driver\n",
            "25/08/07 19:32:45 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 564 ms on 09e87e5f6ba6 (executor driver) (1/1)\n",
            "25/08/07 19:32:45 INFO DAGScheduler: ResultStage 2 (toPandas at /content/app.py:45) finished in 0.607 s\n",
            "25/08/07 19:32:45 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/07 19:32:45 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:32:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
            "25/08/07 19:32:45 INFO DAGScheduler: Job 2 finished: toPandas at /content/app.py:45, took 0.636495 s\n",
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- workclass: string (nullable = true)\n",
            " |-- fnlwgt: integer (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- educational-num: integer (nullable = true)\n",
            " |-- marital-status: string (nullable = true)\n",
            " |-- occupation: string (nullable = true)\n",
            " |-- relationship: string (nullable = true)\n",
            " |-- race: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- capital-gain: integer (nullable = true)\n",
            " |-- capital-loss: integer (nullable = true)\n",
            " |-- hours-per-week: integer (nullable = true)\n",
            " |-- native-country: string (nullable = true)\n",
            " |-- income: string (nullable = true)\n",
            "\n",
            "25/08/07 19:33:07 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
            "25/08/07 19:33:07 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
            "25/08/07 19:33:07 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/07 19:33:07 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#62, None)) > 0)\n",
            "25/08/07 19:33:07 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.1 KiB, free 433.7 MiB)\n",
            "25/08/07 19:33:07 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.7 MiB)\n",
            "25/08/07 19:33:07 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 09e87e5f6ba6:35457 (size: 34.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:07 INFO SparkContext: Created broadcast 6 from csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:33:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4760336 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/07 19:33:07 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:33:07 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "25/08/07 19:33:07 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)\n",
            "25/08/07 19:33:07 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/07 19:33:07 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:33:07 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[16] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/08/07 19:33:07 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.5 KiB, free 433.6 MiB)\n",
            "25/08/07 19:33:07 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 433.6 MiB)\n",
            "25/08/07 19:33:07 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 09e87e5f6ba6:35457 (size: 6.4 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:07 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:33:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/07 19:33:07 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
            "25/08/07 19:33:07 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (09e87e5f6ba6, executor driver, partition 0, PROCESS_LOCAL, 8203 bytes) \n",
            "25/08/07 19:33:07 INFO Executor: Running task 0.0 in stage 3.0 (TID 4)\n",
            "25/08/07 19:33:07 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 0-4760336, partition values: [empty row]\n",
            "25/08/07 19:33:07 INFO Executor: Finished task 0.0 in stage 3.0 (TID 4). 1755 bytes result sent to driver\n",
            "25/08/07 19:33:07 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 34 ms on 09e87e5f6ba6 (executor driver) (1/1)\n",
            "25/08/07 19:33:07 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:33:07 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.057 s\n",
            "25/08/07 19:33:07 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/07 19:33:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
            "25/08/07 19:33:07 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 0.071560 s\n",
            "25/08/07 19:33:07 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/07 19:33:07 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/07 19:33:07 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 200.1 KiB, free 433.4 MiB)\n",
            "25/08/07 19:33:07 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)\n",
            "25/08/07 19:33:07 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 09e87e5f6ba6:35457 (size: 34.5 KiB, free: 434.2 MiB)\n",
            "25/08/07 19:33:07 INFO SparkContext: Created broadcast 8 from csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:33:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4760336 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/07 19:33:07 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:33:07 INFO DAGScheduler: Got job 4 (csv at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
            "25/08/07 19:33:07 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)\n",
            "25/08/07 19:33:07 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/07 19:33:07 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:33:08 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[22] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/08/07 19:33:08 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 27.9 KiB, free 433.4 MiB)\n",
            "25/08/07 19:33:08 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 433.4 MiB)\n",
            "25/08/07 19:33:08 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 09e87e5f6ba6:35457 (size: 12.9 KiB, free: 434.2 MiB)\n",
            "25/08/07 19:33:08 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:33:08 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[22] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/07 19:33:08 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks resource profile 0\n",
            "25/08/07 19:33:08 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (09e87e5f6ba6, executor driver, partition 0, PROCESS_LOCAL, 8203 bytes) \n",
            "25/08/07 19:33:08 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 6) (09e87e5f6ba6, executor driver, partition 1, PROCESS_LOCAL, 8203 bytes) \n",
            "25/08/07 19:33:08 INFO Executor: Running task 1.0 in stage 4.0 (TID 6)\n",
            "25/08/07 19:33:08 INFO Executor: Running task 0.0 in stage 4.0 (TID 5)\n",
            "25/08/07 19:33:08 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 0-4760336, partition values: [empty row]\n",
            "25/08/07 19:33:08 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 4760336-5326368, partition values: [empty row]\n",
            "25/08/07 19:33:08 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 09e87e5f6ba6:35457 in memory (size: 6.4 KiB, free: 434.2 MiB)\n",
            "25/08/07 19:33:08 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 09e87e5f6ba6:35457 in memory (size: 6.8 KiB, free: 434.2 MiB)\n",
            "25/08/07 19:33:08 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 09e87e5f6ba6:35457 in memory (size: 12.9 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:08 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 09e87e5f6ba6:35457 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:08 INFO Executor: Finished task 1.0 in stage 4.0 (TID 6). 1676 bytes result sent to driver\n",
            "25/08/07 19:33:08 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 6) in 402 ms on 09e87e5f6ba6 (executor driver) (1/2)\n",
            "25/08/07 19:33:08 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 09e87e5f6ba6:35457 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:08 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 09e87e5f6ba6:35457 in memory (size: 34.5 KiB, free: 434.4 MiB)\n",
            "25/08/07 19:33:08 INFO Executor: Finished task 0.0 in stage 4.0 (TID 5). 1676 bytes result sent to driver\n",
            "25/08/07 19:33:08 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 884 ms on 09e87e5f6ba6 (executor driver) (2/2)\n",
            "25/08/07 19:33:08 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:33:08 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 0.925 s\n",
            "25/08/07 19:33:08 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/07 19:33:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
            "25/08/07 19:33:08 INFO DAGScheduler: Job 4 finished: csv at NativeMethodAccessorImpl.java:0, took 0.941344 s\n",
            "25/08/07 19:33:09 INFO FileSourceStrategy: Pushed Filters: IsNotNull(age),LessThan(age,40)\n",
            "25/08/07 19:33:09 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(age#79),(age#79 < 40)\n",
            "25/08/07 19:33:09 INFO CodeGenerator: Code generated in 32.776748 ms\n",
            "25/08/07 19:33:09 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 200.0 KiB, free 433.9 MiB)\n",
            "25/08/07 19:33:09 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n",
            "25/08/07 19:33:09 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 09e87e5f6ba6:35457 (size: 34.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:09 INFO SparkContext: Created broadcast 10 from count at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:33:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4760336 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/07 19:33:09 INFO DAGScheduler: Registering RDD 26 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
            "25/08/07 19:33:09 INFO DAGScheduler: Got map stage job 5 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
            "25/08/07 19:33:09 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (count at NativeMethodAccessorImpl.java:0)\n",
            "25/08/07 19:33:09 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/07 19:33:09 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:33:09 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/08/07 19:33:09 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 19.1 KiB, free 433.9 MiB)\n",
            "25/08/07 19:33:09 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 9.3 KiB, free 433.9 MiB)\n",
            "25/08/07 19:33:09 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 09e87e5f6ba6:35457 (size: 9.3 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:09 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:33:09 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/07 19:33:09 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\n",
            "25/08/07 19:33:09 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 7) (09e87e5f6ba6, executor driver, partition 0, PROCESS_LOCAL, 8192 bytes) \n",
            "25/08/07 19:33:09 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 8) (09e87e5f6ba6, executor driver, partition 1, PROCESS_LOCAL, 8192 bytes) \n",
            "25/08/07 19:33:09 INFO Executor: Running task 0.0 in stage 5.0 (TID 7)\n",
            "25/08/07 19:33:09 INFO Executor: Running task 1.0 in stage 5.0 (TID 8)\n",
            "25/08/07 19:33:09 INFO CodeGenerator: Code generated in 42.142506 ms\n",
            "25/08/07 19:33:09 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 0-4760336, partition values: [empty row]\n",
            "25/08/07 19:33:09 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 4760336-5326368, partition values: [empty row]\n",
            "25/08/07 19:33:09 INFO CodeGenerator: Code generated in 24.871219 ms\n",
            "25/08/07 19:33:09 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 09e87e5f6ba6:35457 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:10 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 09e87e5f6ba6:35457 in memory (size: 12.9 KiB, free: 434.4 MiB)\n",
            "25/08/07 19:33:10 INFO CodeGenerator: Code generated in 92.202781 ms\n",
            "25/08/07 19:33:10 INFO Executor: Finished task 1.0 in stage 5.0 (TID 8). 2070 bytes result sent to driver\n",
            "25/08/07 19:33:10 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 8) in 818 ms on 09e87e5f6ba6 (executor driver) (1/2)\n",
            "25/08/07 19:33:11 INFO Executor: Finished task 0.0 in stage 5.0 (TID 7). 2027 bytes result sent to driver\n",
            "25/08/07 19:33:11 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 7) in 1904 ms on 09e87e5f6ba6 (executor driver) (2/2)\n",
            "25/08/07 19:33:11 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:33:11 INFO DAGScheduler: ShuffleMapStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 1.942 s\n",
            "25/08/07 19:33:11 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/07 19:33:11 INFO DAGScheduler: running: Set()\n",
            "25/08/07 19:33:11 INFO DAGScheduler: waiting: Set()\n",
            "25/08/07 19:33:11 INFO DAGScheduler: failed: Set()\n",
            "25/08/07 19:33:11 INFO CodeGenerator: Code generated in 45.194147 ms\n",
            "25/08/07 19:33:11 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:33:11 INFO DAGScheduler: Got job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "25/08/07 19:33:11 INFO DAGScheduler: Final stage: ResultStage 7 (count at NativeMethodAccessorImpl.java:0)\n",
            "25/08/07 19:33:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
            "25/08/07 19:33:11 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:33:11 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/08/07 19:33:11 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)\n",
            "25/08/07 19:33:11 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.1 MiB)\n",
            "25/08/07 19:33:11 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 09e87e5f6ba6:35457 (size: 5.9 KiB, free: 434.4 MiB)\n",
            "25/08/07 19:33:11 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:33:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/07 19:33:11 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
            "25/08/07 19:33:11 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9) (09e87e5f6ba6, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
            "25/08/07 19:33:11 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)\n",
            "25/08/07 19:33:12 INFO ShuffleBlockFetcherIterator: Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/07 19:33:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 37 ms\n",
            "25/08/07 19:33:12 INFO CodeGenerator: Code generated in 55.842002 ms\n",
            "25/08/07 19:33:12 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 4038 bytes result sent to driver\n",
            "25/08/07 19:33:12 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 225 ms on 09e87e5f6ba6 (executor driver) (1/1)\n",
            "25/08/07 19:33:12 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:33:12 INFO DAGScheduler: ResultStage 7 (count at NativeMethodAccessorImpl.java:0) finished in 0.253 s\n",
            "25/08/07 19:33:12 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/07 19:33:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
            "25/08/07 19:33:12 INFO DAGScheduler: Job 6 finished: count at NativeMethodAccessorImpl.java:0, took 0.291931 s\n",
            "25/08/07 19:33:37 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
            "25/08/07 19:33:37 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/07 19:33:37 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/07 19:33:37 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#131, None)) > 0)\n",
            "25/08/07 19:33:37 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 200.1 KiB, free 433.9 MiB)\n",
            "25/08/07 19:33:37 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n",
            "25/08/07 19:33:37 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 09e87e5f6ba6:35457 (size: 34.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:37 INFO SparkContext: Created broadcast 13 from csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:33:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4760336 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/07 19:33:37 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:33:37 INFO DAGScheduler: Got job 7 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "25/08/07 19:33:37 INFO DAGScheduler: Final stage: ResultStage 8 (csv at NativeMethodAccessorImpl.java:0)\n",
            "25/08/07 19:33:37 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/07 19:33:37 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:33:37 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[33] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/08/07 19:33:37 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 13.5 KiB, free 433.9 MiB)\n",
            "25/08/07 19:33:37 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 433.9 MiB)\n",
            "25/08/07 19:33:37 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 09e87e5f6ba6:35457 (size: 6.4 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:37 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:33:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[33] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/07 19:33:37 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
            "25/08/07 19:33:37 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10) (09e87e5f6ba6, executor driver, partition 0, PROCESS_LOCAL, 8203 bytes) \n",
            "25/08/07 19:33:37 INFO Executor: Running task 0.0 in stage 8.0 (TID 10)\n",
            "25/08/07 19:33:37 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 0-4760336, partition values: [empty row]\n",
            "25/08/07 19:33:37 INFO Executor: Finished task 0.0 in stage 8.0 (TID 10). 1755 bytes result sent to driver\n",
            "25/08/07 19:33:37 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 46 ms on 09e87e5f6ba6 (executor driver) (1/1)\n",
            "25/08/07 19:33:37 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:33:37 INFO DAGScheduler: ResultStage 8 (csv at NativeMethodAccessorImpl.java:0) finished in 0.061 s\n",
            "25/08/07 19:33:37 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/07 19:33:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
            "25/08/07 19:33:37 INFO DAGScheduler: Job 7 finished: csv at NativeMethodAccessorImpl.java:0, took 0.073670 s\n",
            "25/08/07 19:33:37 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/07 19:33:37 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/07 19:33:37 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 200.1 KiB, free 433.7 MiB)\n",
            "25/08/07 19:33:37 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.6 MiB)\n",
            "25/08/07 19:33:37 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 09e87e5f6ba6:35457 (size: 34.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:37 INFO SparkContext: Created broadcast 15 from csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:33:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4760336 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/07 19:33:37 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:33:37 INFO DAGScheduler: Got job 8 (csv at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
            "25/08/07 19:33:37 INFO DAGScheduler: Final stage: ResultStage 9 (csv at NativeMethodAccessorImpl.java:0)\n",
            "25/08/07 19:33:37 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/07 19:33:37 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:33:37 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[39] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/08/07 19:33:37 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 27.9 KiB, free 433.6 MiB)\n",
            "25/08/07 19:33:37 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 433.6 MiB)\n",
            "25/08/07 19:33:37 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 09e87e5f6ba6:35457 (size: 12.9 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:37 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:33:37 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[39] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/07 19:33:37 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0\n",
            "25/08/07 19:33:37 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 11) (09e87e5f6ba6, executor driver, partition 0, PROCESS_LOCAL, 8203 bytes) \n",
            "25/08/07 19:33:37 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 12) (09e87e5f6ba6, executor driver, partition 1, PROCESS_LOCAL, 8203 bytes) \n",
            "25/08/07 19:33:37 INFO Executor: Running task 1.0 in stage 9.0 (TID 12)\n",
            "25/08/07 19:33:37 INFO Executor: Running task 0.0 in stage 9.0 (TID 11)\n",
            "25/08/07 19:33:37 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 0-4760336, partition values: [empty row]\n",
            "25/08/07 19:33:37 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 4760336-5326368, partition values: [empty row]\n",
            "25/08/07 19:33:38 INFO Executor: Finished task 1.0 in stage 9.0 (TID 12). 1676 bytes result sent to driver\n",
            "25/08/07 19:33:38 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 12) in 208 ms on 09e87e5f6ba6 (executor driver) (1/2)\n",
            "25/08/07 19:33:38 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 09e87e5f6ba6:35457 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:38 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 09e87e5f6ba6:35457 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:38 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 09e87e5f6ba6:35457 in memory (size: 6.4 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:38 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 09e87e5f6ba6:35457 in memory (size: 9.3 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:38 INFO Executor: Finished task 0.0 in stage 9.0 (TID 11). 1676 bytes result sent to driver\n",
            "25/08/07 19:33:38 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 11) in 752 ms on 09e87e5f6ba6 (executor driver) (2/2)\n",
            "25/08/07 19:33:38 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:33:38 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 09e87e5f6ba6:35457 in memory (size: 34.5 KiB, free: 434.4 MiB)\n",
            "25/08/07 19:33:38 INFO DAGScheduler: ResultStage 9 (csv at NativeMethodAccessorImpl.java:0) finished in 0.789 s\n",
            "25/08/07 19:33:38 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/07 19:33:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
            "25/08/07 19:33:38 INFO DAGScheduler: Job 8 finished: csv at NativeMethodAccessorImpl.java:0, took 0.800973 s\n",
            "25/08/07 19:33:38 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/07 19:33:38 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/07 19:33:39 INFO CodeGenerator: Code generated in 85.179594 ms\n",
            "25/08/07 19:33:39 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 200.0 KiB, free 433.9 MiB)\n",
            "25/08/07 19:33:39 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n",
            "25/08/07 19:33:39 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 09e87e5f6ba6:35457 (size: 34.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:39 INFO SparkContext: Created broadcast 17 from toPandas at /content/app.py:54\n",
            "25/08/07 19:33:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4760336 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/07 19:33:39 INFO DAGScheduler: Registering RDD 43 (toPandas at /content/app.py:54) as input to shuffle 1\n",
            "25/08/07 19:33:39 INFO DAGScheduler: Got map stage job 9 (toPandas at /content/app.py:54) with 2 output partitions\n",
            "25/08/07 19:33:39 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (toPandas at /content/app.py:54)\n",
            "25/08/07 19:33:39 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/07 19:33:39 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:33:39 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[43] at toPandas at /content/app.py:54), which has no missing parents\n",
            "25/08/07 19:33:39 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 39.0 KiB, free 433.9 MiB)\n",
            "25/08/07 19:33:39 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 433.8 MiB)\n",
            "25/08/07 19:33:39 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 09e87e5f6ba6:35457 (size: 18.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:39 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:33:39 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[43] at toPandas at /content/app.py:54) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/07 19:33:39 INFO TaskSchedulerImpl: Adding task set 10.0 with 2 tasks resource profile 0\n",
            "25/08/07 19:33:39 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 13) (09e87e5f6ba6, executor driver, partition 0, PROCESS_LOCAL, 8192 bytes) \n",
            "25/08/07 19:33:39 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 14) (09e87e5f6ba6, executor driver, partition 1, PROCESS_LOCAL, 8192 bytes) \n",
            "25/08/07 19:33:39 INFO Executor: Running task 1.0 in stage 10.0 (TID 14)\n",
            "25/08/07 19:33:39 INFO Executor: Running task 0.0 in stage 10.0 (TID 13)\n",
            "25/08/07 19:33:39 INFO CodeGenerator: Code generated in 99.882443 ms\n",
            "25/08/07 19:33:39 INFO CodeGenerator: Code generated in 24.211344 ms\n",
            "25/08/07 19:33:39 INFO CodeGenerator: Code generated in 11.433768 ms\n",
            "25/08/07 19:33:39 INFO CodeGenerator: Code generated in 23.140569 ms\n",
            "25/08/07 19:33:39 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 0-4760336, partition values: [empty row]\n",
            "25/08/07 19:33:39 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 4760336-5326368, partition values: [empty row]\n",
            "25/08/07 19:33:39 INFO Executor: Finished task 1.0 in stage 10.0 (TID 14). 2722 bytes result sent to driver\n",
            "25/08/07 19:33:39 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 14) in 551 ms on 09e87e5f6ba6 (executor driver) (1/2)\n",
            "25/08/07 19:33:40 INFO Executor: Finished task 0.0 in stage 10.0 (TID 13). 2722 bytes result sent to driver\n",
            "25/08/07 19:33:40 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 13) in 772 ms on 09e87e5f6ba6 (executor driver) (2/2)\n",
            "25/08/07 19:33:40 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:33:40 INFO DAGScheduler: ShuffleMapStage 10 (toPandas at /content/app.py:54) finished in 0.802 s\n",
            "25/08/07 19:33:40 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/07 19:33:40 INFO DAGScheduler: running: Set()\n",
            "25/08/07 19:33:40 INFO DAGScheduler: waiting: Set()\n",
            "25/08/07 19:33:40 INFO DAGScheduler: failed: Set()\n",
            "25/08/07 19:33:40 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/07 19:33:40 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
            "25/08/07 19:33:40 INFO CodeGenerator: Code generated in 33.030667 ms\n",
            "25/08/07 19:33:40 INFO CodeGenerator: Code generated in 25.458611 ms\n",
            "25/08/07 19:33:40 INFO SparkContext: Starting job: toPandas at /content/app.py:54\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Got job 10 (toPandas at /content/app.py:54) with 1 output partitions\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Final stage: ResultStage 12 (toPandas at /content/app.py:54)\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[48] at toPandas at /content/app.py:54), which has no missing parents\n",
            "25/08/07 19:33:40 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 42.4 KiB, free 433.8 MiB)\n",
            "25/08/07 19:33:40 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 433.8 MiB)\n",
            "25/08/07 19:33:40 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 09e87e5f6ba6:35457 (size: 20.1 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:40 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[48] at toPandas at /content/app.py:54) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/07 19:33:40 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
            "25/08/07 19:33:40 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 15) (09e87e5f6ba6, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
            "25/08/07 19:33:40 INFO Executor: Running task 0.0 in stage 12.0 (TID 15)\n",
            "25/08/07 19:33:40 INFO ShuffleBlockFetcherIterator: Getting 2 (2.2 KiB) non-empty blocks including 2 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/07 19:33:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
            "25/08/07 19:33:40 INFO CodeGenerator: Code generated in 43.971495 ms\n",
            "25/08/07 19:33:40 INFO CodeGenerator: Code generated in 14.387428 ms\n",
            "25/08/07 19:33:40 INFO Executor: Finished task 0.0 in stage 12.0 (TID 15). 5796 bytes result sent to driver\n",
            "25/08/07 19:33:40 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 15) in 126 ms on 09e87e5f6ba6 (executor driver) (1/1)\n",
            "25/08/07 19:33:40 INFO DAGScheduler: ResultStage 12 (toPandas at /content/app.py:54) finished in 0.144 s\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/07 19:33:40 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:33:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Job 10 finished: toPandas at /content/app.py:54, took 0.160410 s\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Registering RDD 49 (toPandas at /content/app.py:54) as input to shuffle 2\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Got map stage job 11 (toPandas at /content/app.py:54) with 1 output partitions\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Final stage: ShuffleMapStage 14 (toPandas at /content/app.py:54)\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[49] at toPandas at /content/app.py:54), which has no missing parents\n",
            "25/08/07 19:33:40 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 43.0 KiB, free 433.7 MiB)\n",
            "25/08/07 19:33:40 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 20.5 KiB, free 433.7 MiB)\n",
            "25/08/07 19:33:40 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 09e87e5f6ba6:35457 (size: 20.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:40 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[49] at toPandas at /content/app.py:54) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/07 19:33:40 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
            "25/08/07 19:33:40 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 16) (09e87e5f6ba6, executor driver, partition 0, NODE_LOCAL, 7604 bytes) \n",
            "25/08/07 19:33:40 INFO Executor: Running task 0.0 in stage 14.0 (TID 16)\n",
            "25/08/07 19:33:40 INFO CodeGenerator: Code generated in 11.824087 ms\n",
            "25/08/07 19:33:40 INFO ShuffleBlockFetcherIterator: Getting 2 (2.2 KiB) non-empty blocks including 2 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/07 19:33:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
            "25/08/07 19:33:40 INFO Executor: Finished task 0.0 in stage 14.0 (TID 16). 5317 bytes result sent to driver\n",
            "25/08/07 19:33:40 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 16) in 138 ms on 09e87e5f6ba6 (executor driver) (1/1)\n",
            "25/08/07 19:33:40 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:33:40 INFO DAGScheduler: ShuffleMapStage 14 (toPandas at /content/app.py:54) finished in 0.183 s\n",
            "25/08/07 19:33:40 INFO DAGScheduler: looking for newly runnable stages\n",
            "25/08/07 19:33:40 INFO DAGScheduler: running: Set()\n",
            "25/08/07 19:33:40 INFO DAGScheduler: waiting: Set()\n",
            "25/08/07 19:33:40 INFO DAGScheduler: failed: Set()\n",
            "25/08/07 19:33:40 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
            "25/08/07 19:33:40 INFO CodeGenerator: Code generated in 16.821417 ms\n",
            "25/08/07 19:33:40 INFO SparkContext: Starting job: toPandas at /content/app.py:54\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Got job 12 (toPandas at /content/app.py:54) with 1 output partitions\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Final stage: ResultStage 17 (toPandas at /content/app.py:54)\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[52] at toPandas at /content/app.py:54), which has no missing parents\n",
            "25/08/07 19:33:40 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 41.0 KiB, free 433.7 MiB)\n",
            "25/08/07 19:33:40 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 433.7 MiB)\n",
            "25/08/07 19:33:40 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 09e87e5f6ba6:35457 (size: 19.7 KiB, free: 434.2 MiB)\n",
            "25/08/07 19:33:40 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[52] at toPandas at /content/app.py:54) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/07 19:33:40 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
            "25/08/07 19:33:40 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (09e87e5f6ba6, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
            "25/08/07 19:33:40 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)\n",
            "25/08/07 19:33:40 INFO ShuffleBlockFetcherIterator: Getting 1 (1256.0 B) non-empty blocks including 1 (1256.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
            "25/08/07 19:33:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
            "25/08/07 19:33:40 INFO CodeGenerator: Code generated in 23.500987 ms\n",
            "25/08/07 19:33:40 INFO CodeGenerator: Code generated in 9.498736 ms\n",
            "25/08/07 19:33:40 INFO CodeGenerator: Code generated in 17.454371 ms\n",
            "25/08/07 19:33:40 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 6916 bytes result sent to driver\n",
            "25/08/07 19:33:40 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 126 ms on 09e87e5f6ba6 (executor driver) (1/1)\n",
            "25/08/07 19:33:40 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:33:40 INFO DAGScheduler: ResultStage 17 (toPandas at /content/app.py:54) finished in 0.145 s\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/07 19:33:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
            "25/08/07 19:33:40 INFO DAGScheduler: Job 12 finished: toPandas at /content/app.py:54, took 0.150902 s\n",
            "25/08/07 19:33:53 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.\n",
            "25/08/07 19:33:53 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
            "25/08/07 19:33:53 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/07 19:33:53 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#201, None)) > 0)\n",
            "25/08/07 19:33:53 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 200.1 KiB, free 433.5 MiB)\n",
            "25/08/07 19:33:53 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.4 MiB)\n",
            "25/08/07 19:33:53 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 09e87e5f6ba6:35457 (size: 34.5 KiB, free: 434.2 MiB)\n",
            "25/08/07 19:33:53 INFO SparkContext: Created broadcast 22 from csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:33:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4760336 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/07 19:33:53 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Got job 13 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Final stage: ResultStage 18 (csv at NativeMethodAccessorImpl.java:0)\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[56] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/08/07 19:33:53 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 13.5 KiB, free 433.4 MiB)\n",
            "25/08/07 19:33:53 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 433.4 MiB)\n",
            "25/08/07 19:33:53 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 09e87e5f6ba6:35457 (size: 6.4 KiB, free: 434.2 MiB)\n",
            "25/08/07 19:33:53 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[56] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "25/08/07 19:33:53 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
            "25/08/07 19:33:53 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18) (09e87e5f6ba6, executor driver, partition 0, PROCESS_LOCAL, 8203 bytes) \n",
            "25/08/07 19:33:53 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)\n",
            "25/08/07 19:33:53 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 0-4760336, partition values: [empty row]\n",
            "25/08/07 19:33:53 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 1755 bytes result sent to driver\n",
            "25/08/07 19:33:53 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 27 ms on 09e87e5f6ba6 (executor driver) (1/1)\n",
            "25/08/07 19:33:53 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:33:53 INFO DAGScheduler: ResultStage 18 (csv at NativeMethodAccessorImpl.java:0) finished in 0.038 s\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/07 19:33:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Job 13 finished: csv at NativeMethodAccessorImpl.java:0, took 0.044880 s\n",
            "25/08/07 19:33:53 INFO FileSourceStrategy: Pushed Filters: \n",
            "25/08/07 19:33:53 INFO FileSourceStrategy: Post-Scan Filters: \n",
            "25/08/07 19:33:53 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 200.1 KiB, free 433.2 MiB)\n",
            "25/08/07 19:33:53 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.2 MiB)\n",
            "25/08/07 19:33:53 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 09e87e5f6ba6:35457 (size: 34.5 KiB, free: 434.2 MiB)\n",
            "25/08/07 19:33:53 INFO SparkContext: Created broadcast 24 from csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:33:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4760336 bytes, open cost is considered as scanning 4194304 bytes.\n",
            "25/08/07 19:33:53 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 09e87e5f6ba6:35457 in memory (size: 20.1 KiB, free: 434.2 MiB)\n",
            "25/08/07 19:33:53 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Got job 14 (csv at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Final stage: ResultStage 19 (csv at NativeMethodAccessorImpl.java:0)\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Missing parents: List()\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[62] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/08/07 19:33:53 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 09e87e5f6ba6:35457 in memory (size: 19.7 KiB, free: 434.2 MiB)\n",
            "25/08/07 19:33:53 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 27.9 KiB, free 433.2 MiB)\n",
            "25/08/07 19:33:53 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 433.3 MiB)\n",
            "25/08/07 19:33:53 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 09e87e5f6ba6:35457 (size: 12.9 KiB, free: 434.2 MiB)\n",
            "25/08/07 19:33:53 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1585\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 19 (MapPartitionsRDD[62] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
            "25/08/07 19:33:53 INFO TaskSchedulerImpl: Adding task set 19.0 with 2 tasks resource profile 0\n",
            "25/08/07 19:33:53 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (09e87e5f6ba6, executor driver, partition 0, PROCESS_LOCAL, 8203 bytes) \n",
            "25/08/07 19:33:53 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 20) (09e87e5f6ba6, executor driver, partition 1, PROCESS_LOCAL, 8203 bytes) \n",
            "25/08/07 19:33:53 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)\n",
            "25/08/07 19:33:53 INFO Executor: Running task 1.0 in stage 19.0 (TID 20)\n",
            "25/08/07 19:33:53 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 0-4760336, partition values: [empty row]\n",
            "25/08/07 19:33:53 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 09e87e5f6ba6:35457 in memory (size: 34.5 KiB, free: 434.2 MiB)\n",
            "25/08/07 19:33:53 INFO FileScanRDD: Reading File path: file:///content/adult.csv, range: 4760336-5326368, partition values: [empty row]\n",
            "25/08/07 19:33:53 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 09e87e5f6ba6:35457 in memory (size: 6.4 KiB, free: 434.2 MiB)\n",
            "25/08/07 19:33:53 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 09e87e5f6ba6:35457 in memory (size: 12.9 KiB, free: 434.2 MiB)\n",
            "25/08/07 19:33:53 INFO Executor: Finished task 1.0 in stage 19.0 (TID 20). 1633 bytes result sent to driver\n",
            "25/08/07 19:33:53 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 20) in 193 ms on 09e87e5f6ba6 (executor driver) (1/2)\n",
            "25/08/07 19:33:53 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 09e87e5f6ba6:35457 in memory (size: 20.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:53 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 09e87e5f6ba6:35457 in memory (size: 34.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:53 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 09e87e5f6ba6:35457 in memory (size: 18.5 KiB, free: 434.3 MiB)\n",
            "25/08/07 19:33:53 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 1633 bytes result sent to driver\n",
            "25/08/07 19:33:53 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 349 ms on 09e87e5f6ba6 (executor driver) (2/2)\n",
            "25/08/07 19:33:53 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
            "25/08/07 19:33:53 INFO DAGScheduler: ResultStage 19 (csv at NativeMethodAccessorImpl.java:0) finished in 0.370 s\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/08/07 19:33:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
            "25/08/07 19:33:53 INFO DAGScheduler: Job 14 finished: csv at NativeMethodAccessorImpl.java:0, took 0.380053 s\n",
            "\u001b[31m──\u001b[0m\u001b[31m────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────────\u001b[0m\u001b[31m──\u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mexec_code.py\u001b[0m: \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[94m128\u001b[0m in \u001b[92mexec_func_with_error_handling\u001b[0m                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mscript_runner\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m669\u001b[0m in \u001b[92mcode_to_exec\u001b[0m                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/content/\u001b[0m\u001b[1;33mapp.py\u001b[0m:\u001b[94m57\u001b[0m in \u001b[92m<module>\u001b[0m                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 54 \u001b[0m\u001b[2m│   │   \u001b[0mst.dataframe(edu_counts.toPandas())                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 55 \u001b[0m\u001b[2m│   \u001b[0m                                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 56 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m task == \u001b[33m\"\u001b[0m\u001b[33mLoad to MySQL and Verify\u001b[0m\u001b[33m\"\u001b[0m:                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 57 \u001b[2m│   │   \u001b[0m\u001b[1;4mdf.write.mode(\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33moverwrite\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m).jdbc(url=jdbc_url, table=\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mAdult\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m, propertie\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 58 \u001b[0m\u001b[2m│   │   \u001b[0mverified = spark.read.jdbc(url=jdbc_url, table=\u001b[33m\"\u001b[0m\u001b[33mAdult\u001b[0m\u001b[33m\"\u001b[0m, properties=pro \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 59 \u001b[0m\u001b[2m│   │   \u001b[0mst.dataframe(verified.toPandas())                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 60 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/pyspark/sql/\u001b[0m\u001b[1;33mreadwriter.py\u001b[0m:\u001b[94m1984\u001b[0m in \u001b[92mjdbc\u001b[0m       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1981 \u001b[0m\u001b[2m│   │   \u001b[0m)()                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1982 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m k \u001b[95min\u001b[0m properties:                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1983 \u001b[0m\u001b[2m│   │   │   \u001b[0mjprop.setProperty(k, properties[k])                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1984 \u001b[2m│   │   \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.mode(mode)._jwrite.jdbc(url, table, jprop)\u001b[0m                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1985 \u001b[0m                                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1986 \u001b[0m                                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1987 \u001b[0m\u001b[94mclass\u001b[0m\u001b[90m \u001b[0m\u001b[4;92mDataFrameWriterV2\u001b[0m:                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/py4j/\u001b[0m\u001b[1;33mjava_gateway.py\u001b[0m:\u001b[94m1322\u001b[0m in \u001b[92m__call__\u001b[0m        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1319 \u001b[0m\u001b[2m│   │   │   \u001b[0mproto.END_COMMAND_PART                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1320 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1321 \u001b[0m\u001b[2m│   │   \u001b[0manswer = \u001b[96mself\u001b[0m.gateway_client.send_command(command)                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m1322 \u001b[2m│   │   \u001b[0mreturn_value = \u001b[1;4mget_return_value(\u001b[0m                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1323 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4manswer, \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.gateway_client, \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.target_id, \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.name)\u001b[0m           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1324 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m1325 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m temp_arg \u001b[95min\u001b[0m temp_args:                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/\u001b[0m\u001b[1;33mcaptured.py\u001b[0m:\u001b[94m179\u001b[0m in \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mdeco\u001b[0m                                                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m176 \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mcapture_sql_exception\u001b[0m(f: Callable[..., Any]) -> Callable[..., Any]:        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m177 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mdeco\u001b[0m(*a: Any, **kw: Any) -> Any:                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m178 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m179 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m f(*a, **kw)                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m180 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m Py4JJavaError \u001b[94mas\u001b[0m e:                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m181 \u001b[0m\u001b[2m│   │   │   \u001b[0mconverted = convert_exception(e.java_exception)                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m182 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(converted, UnknownException):                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/py4j/\u001b[0m\u001b[1;33mprotocol.py\u001b[0m:\u001b[94m326\u001b[0m in \u001b[92mget_return_value\u001b[0m     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m323 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mtype\u001b[0m = answer[\u001b[94m1\u001b[0m]                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m324 \u001b[0m\u001b[2m│   │   │   \u001b[0mvalue = OUTPUT_CONVERTER[\u001b[96mtype\u001b[0m](answer[\u001b[94m2\u001b[0m:], gateway_client)         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m325 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m answer[\u001b[94m1\u001b[0m] == REFERENCE_TYPE:                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m326 \u001b[2m│   │   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m Py4JJavaError(\u001b[0m                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m327 \u001b[0m\u001b[1;2;4m│   │   │   │   │   \u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mAn error occurred while calling \u001b[0m\u001b[1;4;33m{0}\u001b[0m\u001b[1;4;33m{1}\u001b[0m\u001b[1;4;33m{2}\u001b[0m\u001b[1;4;33m.\u001b[0m\u001b[1;4;33m\\n\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m.\u001b[0m            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m328 \u001b[0m\u001b[1;2;4m│   │   │   │   │   \u001b[0m\u001b[1;4;96mformat\u001b[0m\u001b[1;4m(target_id, \u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33m.\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m, name), value)\u001b[0m                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m329 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[1;91mPy4JJavaError: \u001b[0mAn error occurred while calling o81.jdbc.\n",
            ": java.lang.ClassNotFoundException: com.mysql.cj.jdbc.Driver\n",
            "        at java.base/\u001b[1;35mjava.net.URLClassLoader.findClass\u001b[0m\u001b[1m(\u001b[0mURLClassLoader.jav\u001b[1;92ma:476\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at java.base/\u001b[1;35mjava.lang.ClassLoader.loadClass\u001b[0m\u001b[1m(\u001b[0mClassLoader.jav\u001b[1;92ma:594\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at java.base/\u001b[1;35mjava.lang.ClassLoader.loadClass\u001b[0m\u001b[1m(\u001b[0mClassLoader.jav\u001b[1;92ma:527\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$\u001b[1;35m.register\u001b[0m\u001b[1m(\u001b[0mDriverRegistry.\n",
            "scal\u001b[1;92ma:46\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mJDBCO\n",
            "ptions.scal\u001b[1;92ma:103\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$\u001b[1;36m1\u001b[0m$\u001b[1;35madapt\u001b[0m\n",
            "\u001b[1;35med\u001b[0m\u001b[1m(\u001b[0mJDBCOptions.scal\u001b[1;92ma:103\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \u001b[1;35mscala.Option.foreach\u001b[0m\u001b[1m(\u001b[0mOption.scal\u001b[1;92ma:407\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.\u001b[1m<\u001b[0m\u001b[1;95minit\u001b[0m\u001b[39m>\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mJDBCOptions.scal\u001b[0m\u001b[1;92ma:103\u001b[0m\n",
            "\u001b[1;39m)\u001b[0m\n",
            "\u001b[39m        \u001b[0m\u001b[39mat \u001b[0m\n",
            "\u001b[39morg.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>\u001b[0m\u001b[1;39m(\u001b[0m\u001b[39mJDBCOptions.sc\u001b[0m\n",
            "\u001b[39mal\u001b[0m\u001b[1;92ma:254\u001b[0m\u001b[1;39m)\u001b[0m\n",
            "\u001b[39m        \u001b[0m\u001b[39mat \u001b[0m\n",
            "\u001b[39morg.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init\u001b[0m\u001b[1m>\u001b[0m\u001b[1m(\u001b[0mJDBCOptions.sc\n",
            "al\u001b[1;92ma:258\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "\u001b[1;35morg.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation\u001b[0m\u001b[1m(\u001b[0mJdbc\n",
            "RelationProvider.scal\u001b[1;92ma:47\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "\u001b[1;35morg.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run\u001b[0m\u001b[1m(\u001b[0mSaveIntoDataSou\n",
            "rceCommand.scal\u001b[1;92ma:48\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$\u001b[1;35mlzycompute\u001b[0m\u001b[1m(\u001b[0mc\n",
            "ommands.scal\u001b[1;92ma:75\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "\u001b[1;35morg.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult\u001b[0m\u001b[1m(\u001b[0mcommands.sca\n",
            "l\u001b[1;92ma:73\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "\u001b[1;35morg.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect\u001b[0m\u001b[1m(\u001b[0mcommands.scal\u001b[1;92ma\u001b[0m\n",
            "\u001b[1;92m:84\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$\u001b[1;36m1\u001b[0m.$anonfun\n",
            "$applyOrElse$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mQueryExecution.scal\u001b[1;92ma:107\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$\u001b[1;35m6\u001b[0m\u001b[1m(\u001b[0mSQLExecution.\n",
            "scal\u001b[1;92ma:125\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.execution.SQLExecution$\u001b[1;35m.withSQLConfPropagated\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:20\u001b[0m\n",
            "\u001b[1;92m1\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mSQLExecution.\n",
            "scal\u001b[1;92ma:108\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \u001b[1;35morg.apache.spark.sql.SparkSession.withActive\u001b[0m\u001b[1m(\u001b[0mSparkSession.scal\u001b[1;92ma:900\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.execution.SQLExecution$\u001b[1;35m.withNewExecutionId\u001b[0m\u001b[1m(\u001b[0mSQLExecution.scal\u001b[1;92ma:66\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$\u001b[1;35m1.applyOrE\u001b[0m\n",
            "\u001b[1;35mlse\u001b[0m\u001b[1m(\u001b[0mQueryExecution.scal\u001b[1;92ma:107\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$\u001b[1;35m1.applyOrE\u001b[0m\n",
            "\u001b[1;35mlse\u001b[0m\u001b[1m(\u001b[0mQueryExecution.scal\u001b[1;92ma:98\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$\u001b[1;35m1\u001b[0m\u001b[1m(\u001b[0mTreeNod\n",
            "e.scal\u001b[1;92ma:461\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.catalyst.trees.CurrentOrigin$\u001b[1;35m.withOrigin\u001b[0m\u001b[1m(\u001b[0morigin.scal\u001b[1;92ma:76\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "\u001b[1;35morg.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning\u001b[0m\u001b[1m(\u001b[0mTreeNode.scal\u001b[1;92ma:461\u001b[0m\n",
            "\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$pl\n",
            "ans$logical$AnalysisHelper$$super$\u001b[1;35mtransformDownWithPruning\u001b[0m\u001b[1m(\u001b[0mLogicalPlan.scal\u001b[1;92ma:32\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "\u001b[1;35morg.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning\u001b[0m\u001b[1m(\u001b[0mAnal\n",
            "ysisHelper.scal\u001b[1;92ma:267\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$\u001b[1m(\u001b[0mAna\n",
            "lysisHelper.scal\u001b[1;92ma:263\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "\u001b[1;35morg.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning\u001b[0m\u001b[1m(\u001b[0mLogical\n",
            "Plan.scal\u001b[1;92ma:32\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "\u001b[1;35morg.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning\u001b[0m\u001b[1m(\u001b[0mLogical\n",
            "Plan.scal\u001b[1;92ma:32\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "\u001b[1;35morg.apache.spark.sql.catalyst.trees.TreeNode.transformDown\u001b[0m\u001b[1m(\u001b[0mTreeNode.scal\u001b[1;92ma:437\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "\u001b[1;35morg.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands\u001b[0m\u001b[1m(\u001b[0mQueryExecution.scal\n",
            "\u001b[1;92ma:98\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "org.apache.spark.sql.execution.QueryExecution.commandExecuted$\u001b[1;35mlzycompute\u001b[0m\u001b[1m(\u001b[0mQueryExecution.\n",
            "scal\u001b[1;92ma:85\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "\u001b[1;35morg.apache.spark.sql.execution.QueryExecution.commandExecuted\u001b[0m\u001b[1m(\u001b[0mQueryExecution.scal\u001b[1;92ma:83\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "\u001b[1;35morg.apache.spark.sql.execution.QueryExecution.assertCommandExecuted\u001b[0m\u001b[1m(\u001b[0mQueryExecution.scal\u001b[1;92ma\u001b[0m\n",
            "\u001b[1;92m:142\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \u001b[1;35morg.apache.spark.sql.DataFrameWriter.runCommand\u001b[0m\u001b[1m(\u001b[0mDataFrameWriter.scal\u001b[1;92ma:859\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "\u001b[1;35morg.apache.spark.sql.DataFrameWriter.saveToV1Source\u001b[0m\u001b[1m(\u001b[0mDataFrameWriter.scal\u001b[1;92ma:388\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \u001b[1;35morg.apache.spark.sql.DataFrameWriter.saveInternal\u001b[0m\u001b[1m(\u001b[0mDataFrameWriter.scal\u001b[1;92ma:361\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \u001b[1;35morg.apache.spark.sql.DataFrameWriter.save\u001b[0m\u001b[1m(\u001b[0mDataFrameWriter.scal\u001b[1;92ma:248\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \u001b[1;35morg.apache.spark.sql.DataFrameWriter.jdbc\u001b[0m\u001b[1m(\u001b[0mDataFrameWriter.scal\u001b[1;92ma:756\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at java.base/\u001b[1;35mjdk.internal.reflect.NativeMethodAccessorImpl.invoke0\u001b[0m\u001b[1m(\u001b[0mNative \n",
            "Method\u001b[1m)\u001b[0m\n",
            "        at \n",
            "java.base/\u001b[1;35mjdk.internal.reflect.NativeMethodAccessorImpl.invoke\u001b[0m\u001b[1m(\u001b[0mNativeMethodAccessorImpl.\n",
            "jav\u001b[1;92ma:62\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \n",
            "java.base/\u001b[1;35mjdk.internal.reflect.DelegatingMethodAccessorImpl.invoke\u001b[0m\u001b[1m(\u001b[0mDelegatingMethodAcces\n",
            "sorImpl.jav\u001b[1;92ma:43\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at java.base/\u001b[1;35mjava.lang.reflect.Method.invoke\u001b[0m\u001b[1m(\u001b[0mMethod.jav\u001b[1;92ma:566\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \u001b[1;35mpy4j.reflection.MethodInvoker.invoke\u001b[0m\u001b[1m(\u001b[0mMethodInvoker.jav\u001b[1;92ma:244\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \u001b[1;35mpy4j.reflection.ReflectionEngine.invoke\u001b[0m\u001b[1m(\u001b[0mReflectionEngine.jav\u001b[1;92ma:374\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \u001b[1;35mpy4j.Gateway.invoke\u001b[0m\u001b[1m(\u001b[0mGateway.jav\u001b[1;92ma:282\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \u001b[1;35mpy4j.commands.AbstractCommand.invokeMethod\u001b[0m\u001b[1m(\u001b[0mAbstractCommand.jav\u001b[1;92ma:132\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \u001b[1;35mpy4j.commands.CallCommand.execute\u001b[0m\u001b[1m(\u001b[0mCallCommand.jav\u001b[1;92ma:79\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \u001b[1;35mpy4j.ClientServerConnection.waitForCommands\u001b[0m\u001b[1m(\u001b[0mClientServerConnection.jav\u001b[1;92ma:182\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at \u001b[1;35mpy4j.ClientServerConnection.run\u001b[0m\u001b[1m(\u001b[0mClientServerConnection.jav\u001b[1;92ma:106\u001b[0m\u001b[1m)\u001b[0m\n",
            "        at java.base/\u001b[1;35mjava.lang.Thread.run\u001b[0m\u001b[1m(\u001b[0mThread.jav\u001b[1;92ma:829\u001b[0m\u001b[1m)\u001b[0m\n",
            "\n",
            "\u001b[31m──\u001b[0m\u001b[31m────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────────\u001b[0m\u001b[31m──\u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/neo4j/_sync/io/\u001b[0m\u001b[1;33m_bolt_socket.py\u001b[0m:\u001b[94m328\u001b[0m in        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mconnect\u001b[0m                                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m325 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mtcp_timeout = deadline_timeout                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m326 \u001b[0m\u001b[2m│   │   │   \u001b[0ms = \u001b[94mNone\u001b[0m                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m327 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m328 \u001b[2m│   │   │   │   \u001b[0ms = \u001b[96mcls\u001b[0m._connect_secure(                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m329 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mresolved_address, tcp_timeout, keep_alive, ssl_context     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m330 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m331 \u001b[0m\u001b[2m│   │   │   │   \u001b[0magreed_version, handshake, response = s._handshake(            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/neo4j/_async_compat/network/\u001b[0m\u001b[1;33m_bolt_socket.py\u001b[0m: \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[94m415\u001b[0m in \u001b[92m_connect_secure\u001b[0m                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m412 \u001b[0m\u001b[2m│   │   │   │   \u001b[0ms.setsockopt(SOL_SOCKET, SO_KEEPALIVE, keep_alive)             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m413 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mexcept\u001b[0m SocketTimeout:                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m414 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlog.debug(\u001b[33m\"\u001b[0m\u001b[33m[#0000]  S: <TIMEOUT> \u001b[0m\u001b[33m%s\u001b[0m\u001b[33m\"\u001b[0m, resolved_address)        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m415 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m ServiceUnavailable(                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m416 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mTimed out trying to establish connection to \u001b[0m\u001b[33m\"\u001b[0m             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m417 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mresolved_address\u001b[33m!r}\u001b[0m\u001b[33m\"\u001b[0m                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m418 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m) \u001b[94mfrom\u001b[0m\u001b[90m \u001b[0m\u001b[94mNone\u001b[0m                                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[1;91mServiceUnavailable: \u001b[0mTimed out trying to establish connection to \n",
            "\u001b[1;35mResolvedIPv4Address\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'52.21.191.52'\u001b[0m, \u001b[1;36m7687\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
            "\n",
            "\u001b[3mThe above exception was the direct cause of the following exception:\u001b[0m\n",
            "\n",
            "\u001b[31m──\u001b[0m\u001b[31m────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────────\u001b[0m\u001b[31m──\u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mexec_code.py\u001b[0m: \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[94m128\u001b[0m in \u001b[92mexec_func_with_error_handling\u001b[0m                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/streamlit/runtime/scriptrunner/\u001b[0m\u001b[1;33mscript_runner\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[1;33m.py\u001b[0m:\u001b[94m669\u001b[0m in \u001b[92mcode_to_exec\u001b[0m                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/content/\u001b[0m\u001b[1;33mapp.py\u001b[0m:\u001b[94m73\u001b[0m in \u001b[92m<module>\u001b[0m                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 70 \u001b[0m\u001b[2m│   \u001b[0m                                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 71 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m query_option == \u001b[33m\"\u001b[0m\u001b[33mCount All Nodes\u001b[0m\u001b[33m\"\u001b[0m:                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 72 \u001b[0m\u001b[2m│   │   \u001b[0mq = \u001b[33m\"\u001b[0m\u001b[33mMATCH (n) RETURN count(n) AS nodeCount\u001b[0m\u001b[33m\"\u001b[0m                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 73 \u001b[2m│   │   \u001b[0mst.json(\u001b[1;4mrun_cypher_query(q)\u001b[0m)                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 74 \u001b[0m\u001b[2m│   \u001b[0m                                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 75 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m query_option == \u001b[33m\"\u001b[0m\u001b[33mCreate Ronaldinho & Brazil\u001b[0m\u001b[33m\"\u001b[0m:                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 76 \u001b[0m\u001b[2m│   │   \u001b[0mq = \u001b[33m'''\u001b[0m                                                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/content/\u001b[0m\u001b[1;33mapp.py\u001b[0m:\u001b[94m29\u001b[0m in \u001b[92mrun_cypher_query\u001b[0m                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 26 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 27 \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mrun_cypher_query\u001b[0m(query):                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 28 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mwith\u001b[0m driver.session() \u001b[94mas\u001b[0m session:                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 29 \u001b[2m│   │   \u001b[0mresult = \u001b[1;4msession.run(query)\u001b[0m                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 30 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m [record.data() \u001b[94mfor\u001b[0m record \u001b[95min\u001b[0m result]                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 31 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 32 \u001b[0m\u001b[2m# UI\u001b[0m                                                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/neo4j/_sync/work/\u001b[0m\u001b[1;33msession.py\u001b[0m:\u001b[94m315\u001b[0m in \u001b[92mrun\u001b[0m       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m312 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._auto_result._buffer_all()                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m313 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m314 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m._connection:                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m315 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m._connect(\u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m._config.default_access_mode)\u001b[0m                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m316 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94massert\u001b[0m \u001b[96mself\u001b[0m._connection \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m317 \u001b[0m\u001b[2m│   │   \u001b[0mcx = \u001b[96mself\u001b[0m._connection                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m318 \u001b[0m                                                                               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/neo4j/_sync/work/\u001b[0m\u001b[1;33msession.py\u001b[0m:\u001b[94m136\u001b[0m in \u001b[92m_connect\u001b[0m  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m133 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m access_mode \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m134 \u001b[0m\u001b[2m│   │   │   \u001b[0maccess_mode = \u001b[96mself\u001b[0m._config.default_access_mode                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m135 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m136 \u001b[2m│   │   │   \u001b[0m\u001b[96msuper\u001b[0m()._connect(                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m137 \u001b[0m\u001b[2m│   │   │   │   \u001b[0maccess_mode, auth=\u001b[96mself\u001b[0m._config.auth, **acquire_kwargs          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m138 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m asyncio.CancelledError:                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/neo4j/_sync/work/\u001b[0m\u001b[1;33mworkspace.py\u001b[0m:\u001b[94m199\u001b[0m in         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92m_connect\u001b[0m                                                                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m196 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mdatabase_callback\u001b[0m\u001b[33m\"\u001b[0m: \u001b[96mself\u001b[0m._make_db_resolution_callback(),          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m197 \u001b[0m\u001b[2m│   │   \u001b[0m}                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   │   \u001b[0macquire_kwargs_.update(acquire_kwargs)                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m199 \u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._connection = \u001b[1;4;96mself\u001b[0m\u001b[1;4m._pool.acquire(**acquire_kwargs_)\u001b[0m               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m (                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   │   │   \u001b[0mtarget_db.guessed                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m202 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m._pinned_database                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/neo4j/_sync/io/\u001b[0m\u001b[1;33m_pool.py\u001b[0m:\u001b[94m676\u001b[0m in \u001b[92macquire\u001b[0m       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 673 \u001b[0m\u001b[2m│   │   │   \u001b[0mdatabase,                                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 674 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 675 \u001b[0m\u001b[2m│   │   \u001b[0mdeadline = Deadline.from_timeout_or_deadline(timeout)                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 676 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m._acquire(\u001b[0m                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 677 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.address, auth, deadline, liveness_check_timeout, unprepared\u001b[0m  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 678 \u001b[0m\u001b[1;2;4m│   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                                     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 679 \u001b[0m                                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/neo4j/_sync/io/\u001b[0m\u001b[1;33m_pool.py\u001b[0m:\u001b[94m417\u001b[0m in \u001b[92m_acquire\u001b[0m      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 414 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mdeadline.original_timeout\u001b[33m!r}\u001b[0m\u001b[33ms (timeout)\u001b[0m\u001b[33m\"\u001b[0m           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 415 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m)                                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 416 \u001b[0m\u001b[2m│   │   \u001b[0mlog.debug(\u001b[33m\"\u001b[0m\u001b[33m[#0000]  _: <POOL> trying to hand out new connection\u001b[0m\u001b[33m\"\u001b[0m)     \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 417 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mconnection_creator()\u001b[0m                                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 418 \u001b[0m\u001b[2m│   \u001b[0m                                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 419 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@abc\u001b[0m.abstractmethod                                                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 420 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92macquire\u001b[0m(                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/neo4j/_sync/io/\u001b[0m\u001b[1;33m_pool.py\u001b[0m:\u001b[94m230\u001b[0m in               \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mconnection_creator\u001b[0m                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 227 \u001b[0m\u001b[2m│   │   │   \u001b[0mreleased_reservation = \u001b[94mFalse\u001b[0m                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 228 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 229 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 230 \u001b[2m│   │   │   │   │   \u001b[0mconnection = \u001b[96mself\u001b[0m.opener(                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 231 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0maddress, auth \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m.pool_config.auth, deadline      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 232 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m)                                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 233 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mexcept\u001b[0m ServiceUnavailable:                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/neo4j/_sync/io/\u001b[0m\u001b[1;33m_pool.py\u001b[0m:\u001b[94m637\u001b[0m in \u001b[92mopener\u001b[0m        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 634 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 635 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 636 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mopener\u001b[0m(addr, auth_manager, deadline):                             \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 637 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m Bolt.open(                                                 \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 638 \u001b[0m\u001b[2m│   │   │   │   \u001b[0maddr,                                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 639 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mauth_manager=auth_manager,                                    \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 640 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mdeadline=deadline,                                            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/neo4j/_sync/io/\u001b[0m\u001b[1;33m_bolt.py\u001b[0m:\u001b[94m368\u001b[0m in \u001b[92mopen\u001b[0m          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 365 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m deadline \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 366 \u001b[0m\u001b[2m│   │   │   \u001b[0mdeadline = Deadline(\u001b[94mNone\u001b[0m)                                         \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 367 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m 368 \u001b[2m│   │   \u001b[0ms, protocol_version, handshake, data = BoltSocket.connect(            \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 369 \u001b[0m\u001b[2m│   │   │   \u001b[0maddress,                                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 370 \u001b[0m\u001b[2m│   │   │   \u001b[0mtcp_timeout=pool_config.connection_timeout,                       \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m 371 \u001b[0m\u001b[2m│   │   │   \u001b[0mdeadline=deadline,                                                \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/neo4j/_sync/io/\u001b[0m\u001b[1;33m_bolt_socket.py\u001b[0m:\u001b[94m376\u001b[0m in        \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[92mconnect\u001b[0m                                                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m373 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m374 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                  \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m375 \u001b[0m\u001b[2m│   │   │   \u001b[0merror_strs = \u001b[33m\"\u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m.join(\u001b[96mmap\u001b[0m(\u001b[96mstr\u001b[0m, errors))                           \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m \u001b[31m❱ \u001b[0m376 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m ServiceUnavailable(\u001b[0m                                          \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m377 \u001b[0m\u001b[1;2;4m│   │   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mCouldn\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33mt connect to \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4maddress\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m (resolved to \u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4maddress_strs\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m):\u001b[0m\u001b[1;4;33m\"\u001b[0m \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m378 \u001b[0m\u001b[1;2;4m│   │   │   │   \u001b[0m\u001b[1;4;33mf\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33m\\n\u001b[0m\u001b[1;4;33m{\u001b[0m\u001b[1;4merror_strs\u001b[0m\u001b[1;4;33m}\u001b[0m\u001b[1;4;33m\"\u001b[0m                                              \u001b[31m \u001b[0m\n",
            "\u001b[31m \u001b[0m   \u001b[2m379 \u001b[0m\u001b[1;2;4m│   │   │   \u001b[0m\u001b[1;4m) \u001b[0m\u001b[1;4;94mfrom\u001b[0m\u001b[1;4;90m \u001b[0m\u001b[1;4;96merrors\u001b[0m\u001b[1;4m[\u001b[0m\u001b[1;4;94m0\u001b[0m\u001b[1;4m]\u001b[0m                                                   \u001b[31m \u001b[0m\n",
            "\u001b[31m────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
            "\u001b[1;91mServiceUnavailable: \u001b[0mCouldn't connect to \u001b[1;92m52.21.191.52\u001b[0m:\u001b[1;36m7687\u001b[0m \u001b[1m(\u001b[0mresolved to \n",
            "\u001b[1m(\u001b[0m\u001b[32m'52.21.191.52:7687'\u001b[0m,\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m:\n",
            "Timed out trying to establish connection to \u001b[1;35mResolvedIPv4Address\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'52.21.191.52'\u001b[0m, \u001b[1;36m7687\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}